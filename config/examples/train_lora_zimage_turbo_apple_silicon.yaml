---
job: extension
config:
  # this name will be the folder and filename name
  name: "zimage_turbo_lora_mps"
  process:
    - type: 'sd_trainer'
      # root folder to save training sessions/samples/weights
      training_folder: "output"
      # uncomment to see performance stats in the terminal every N steps
#      performance_log_every: 1000
      device: mps  # Use Apple Silicon Metal Performance Shaders (MPS)
      # Alternative device options:
      # device: mps:0      # Specific MPS device (same as mps on Apple Silicon)
      # device: cuda       # NVIDIA GPU (CUDA)
      # device: cpu        # CPU (slower but universal)

      # Z-Image Turbo Model Configuration
      model:
        name_or_path: "Tongyi-MAI/Z-Image-Turbo"
        # Z-Image specific settings
        quantize: true         # Enable quantization for memory efficiency on Mac
        quantize_te: true      # Quantize text encoder
        low_vram: true         # Enable low VRAM mode
        qtype: "qfloat8"       # Use 8-bit quantization
        # Training adapter for Z-Image Turbo
        assistant_lora_path: "ostris/zimage_turbo_training_adapter/zimage_turbo_training_adapter_v1.safetensors"

        # Apple Silicon optimizations
        layer_offloading: true # Offload layers to save memory
        attention_slice_size: 1 # Slice attention for memory efficiency
        vae_slicing: true      # Enable VAE slicing

      # if a trigger word is specified, it will be added to captions of training data if it does not already exist
      # alternatively, in your captions you can add [trigger] and it will be replaced with the trigger word
#      trigger_word: "ZIT"

      network:
        type: "lora"
        linear: 16
        linear_alpha: 16
        # Note: Z-Image Turbo may not support conv layers in LoRA

      save:
        dtype: float16 # precision to save
        save_every: 250 # save every this many steps
        max_step_saves_to_keep: 4 # how many intermittent saves to keep
        push_to_hub: false #change this to True to push your trained model to Hugging Face.
        # You can either set up a HF_TOKEN env variable or you'll be prompted to log-in
#       hf_repo_id: your-username/your-model-slug
#       hf_private: true #whether the repo is private or public
      datasets:
        # datasets are a folder of images. captions need to be txt files with the same name as the image
        # for instance image2.jpg and image2.txt. Only jpg, jpeg, and png are supported currently
        - folder_path: "/path/to/your/training/images"
          caption_ext: ".txt"
          caption_data: true # captions are in separate .txt files
          # If you have a JSON file with captions, you can use:
          # caption_file: "/path/to/captions.json"
          # caption_data: false
          # You can also use this option to create captions from filenames:
          # caption_data: "filename" # will use the filename as caption
          # Or specify a fixed caption:
          # caption_data: "a photo of [trigger]"

      # Z-Image Turbo Training Settings (Optimized for Apple Silicon)
      train:
        batch_size: 1        # MPS handles memory differently, start with 1
        gradient_accumulation_steps: 8  # Accumulate to simulate larger batch
        precision: "bfloat16"  # Recommended for MPS, or use "float16"
        mixed_precision: true

        # Training schedule
        steps: 1000
        lr: 1e-4
        optimizer: "adamw"
        lr_scheduler: "constant"

        # Z-Image specific settings
        noise_scheduler: "flowmatch"  # Use flowmatch noise scheduler
        timestep_type: "weighted"     # Use weighted timestep sampling
        guidance_scale: 1.0           # Z-Image works well with low guidance
        sample_steps: 8               # Z-Image Turbo uses few steps

        # MPS-specific optimizations
        gradient_checkpointing: true  # Saves memory on Apple Silicon
        unload_text_encoder: false    # Keep text encoder loaded for efficiency

      # Sample settings for Z-Image
      sample:
        sampler: "flowmatch" # Z-Image uses flowmatch sampling
        sample_every: 100    # Sample every N steps
        sample_steps: 8      # Z-Image Turbo uses 8 steps
        width: 512
        height: 512
        guidance_scale: 1.0  # Low guidance for Z-Image
        prompts:
          - "a photo of [trigger]"  # Will be replaced with trigger word if specified
          - "a beautiful landscape with [trigger]"
          - "[trigger] in a cinematic scene"
          - "artistic portrait of [trigger]"
          - "[trigger] in a fantasy setting"

      # Validation (optional but recommended)
      validation:
        validation_prompt: "a photo of [trigger]"
        validation_steps: 200  # Validate every N steps

      # Apple Silicon Memory Management
      memory:
        enable_memory_efficient_attention: true
        use_scaled_dot_product_attention: true

      # Advanced settings for Z-Image
      advanced:
        diff_output_preservation: false
        blank_prompt_preservation: false

# Apple Silicon Performance Tips for Z-Image:
# 1. Use the quantization settings enabled above to reduce memory usage
# 2. The training adapter is required for Z-Image Turbo LoRA training
# 3. Z-Image works best with lower guidance scales (1.0-2.0)
# 4. Use flowmatch sampling for best results
# 5. If you run into memory issues, reduce batch_size to 1 and increase gradient_accumulation_steps
# 6. bfloat16 precision is recommended over float16 for better training stability on Apple Silicon