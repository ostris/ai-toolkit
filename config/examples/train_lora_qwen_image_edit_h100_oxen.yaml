---
job: extension
config:
  # this name will be the folder and filename name
  name: "qwen_image_edit_bloxxxy"
  process:
    - type: 'sd_trainer'
      # root folder to save training sessions/samples/weights
      training_folder: "output"
      # uncomment to see performance stats in the terminal every N steps
#      performance_log_every: 1000
      device: cuda:0
      # if a trigger word is specified, it will be added to captions of training data if it does not already exist
      # alternatively, in your captions you can add [trigger] and it will be replaced with the trigger word
      # Trigger words will not work when caching text embeddings
#      trigger_word: "p3r5on"

      # Oxen experiment tracking configuration (NEW!)
      oxen:
        enabled: true  # Enable Oxen experiment tracking
        repo_id: "ox/QwenImageExp"  # Replace with your Oxen repo ID (namespace/repo_name)
        host: "dev.hub.oxen.ai"  # Oxen host (default: hub.oxen.ai)
        scheme: "https"  # URL scheme (default: https)
        output_dir_base: "experiments"  # Base directory for experiment outputs
        experiment_type: "flux-lora-training"  # Type/category for the experiment
        # fine_tune_id: "flux_experiment_001"  # Optional: Custom ID for this training run
        log_every: 10  # Log metrics every N steps (set to match or be multiple of logging.log_every)

      network:
        type: "lora"
        linear: 16
        linear_alpha: 16

      save:
        dtype: float16 # precision to save
        save_every: 50 # save every this many steps
        max_step_saves_to_keep: 4 # how many intermittent saves to keep

      # Standard logging configuration
      logging:
        log_every: 10  # Log to standard logger every N steps
        verbose: false
        use_wandb: false  # Can use both Oxen and W&B simultaneously if desired

      datasets:
        # datasets are a folder of images. captions need to be txt files with the same name as the image
        # for instance image2.jpg and image2.txt. Only jpg, jpeg, and png are supported currently
        # images will automatically be resized and bucketed into the resolution specified
        # on windows, escape back slashes with another backslash so
        # "C:\\path\\to\\images\\folder"
        - folder_path: "data/Oxen-Character-Simple-Vector-Graphic/data/pairs"
          control_path: "data/Oxen-Character-Simple-Vector-Graphic/data/control"
          caption_ext: "txt"
          # default_caption: "a person" # if caching text embeddings, if you don't have captions, this will get cached
          caption_dropout_rate: 0.05  # will drop out the caption 5% of time
          resolution: [ 512, 768, 1024 ]  # qwen image enjoys multiple resolutions

      train:
        batch_size: 1
        # caching text embeddings is not required for H100
        cache_text_embeddings: false

        steps: 13  # total number of steps to train 500 - 4000 is a good range
        gradient_accumulation: 1
        timestep_type: "weighted"
        train_unet: true
        train_text_encoder: false  # probably won't work with qwen image
        gradient_checkpointing: true  # need the on unless you have a ton of vram
        noise_scheduler: "flowmatch" # for training only
        optimizer: "adamw8bit"
        lr: 2e-4
        # uncomment this to skip the pre training sample
#        skip_first_sample: true
        # uncomment to completely disable sampling
#        disable_sampling: true
        dtype: bf16
      model:
        # huggingface model name or path
        name_or_path: "Qwen/Qwen-Image-Edit"
        arch: "qwen_image_edit"
        quantize: false
        # qtype_te: "qfloat8" Default float8 qquantization
        # to use the ARA use the | pipe to point to hf path, or a local path if you have one.
        # 3bit is required for 32GB
        # qtype: "uint3|qwen_image_edit_torchao_uint3.safetensors"
        # quantize_te: true
        # qtype_te: "qfloat8"
        low_vram: false
      sample:
        sampler: "flowmatch" # must match train.noise_scheduler
        sample_every: 10 # sample every this many steps
        width: 512
        height: 512
        samples:
          - prompt: "training in the gym"
            ctrl_img: "data/Oxen-Character-Simple-Vector-Graphic/data/control/image_0a378be2-95a8-4b6b-b11d-29ead91731bc.jpg"
          # - prompt: "working on a workbench"
          #   ctrl_img: "data/Oxen-Character-Simple-Vector-Graphic/data/control/image_0a378be2-95a8-4b6b-b11d-29ead91731bc.jpg"
          # - prompt: "hammering nails"
          #   ctrl_img: "data/Oxen-Character-Simple-Vector-Graphic/data/control/image_0a378be2-95a8-4b6b-b11d-29ead91731bc.jpg"
          # - prompt: "jumping rope"
          #   ctrl_img: "data/Oxen-Character-Simple-Vector-Graphic/data/control/image_0a378be2-95a8-4b6b-b11d-29ead91731bc.jpg"
          - prompt: "with a crab on his shoulder"
            ctrl_img: "data/Oxen-Character-Simple-Vector-Graphic/data/control/image_0a378be2-95a8-4b6b-b11d-29ead91731bc.jpg"
        neg: ""
        seed: 42
        walk_seed: true
        guidance_scale: 3
        sample_steps: 25
# you can add any additional meta info here. [name] is replaced with config name at top
meta:
  name: "[name]"
  version: '1.0'
