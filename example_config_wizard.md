# Interactive Config Wizard - Usage Guide

The Interactive Config Wizard helps you generate optimized training configurations based on your hardware and training goals. No more guessing at optimal settings!

## Quick Start

Run the wizard from the AI Toolkit directory:

```bash
python -m toolkit.config_wizard
```

Or directly:

```bash
python toolkit/config_wizard.py
```

## What It Does

The wizard will:

1. **Detect your hardware** - GPU, RAM, storage type
2. **Ask about your training** - dataset size, resolution, goals
3. **Calculate optimal settings** - batch size, workers, caching strategy
4. **Generate configuration** - YAML file with explanatory comments
5. **Provide next steps** - How to use the generated config

## Example Session

```
======================================================================
    AI TOOLKIT - INTERACTIVE CONFIGURATION WIZARD
======================================================================

This wizard will help you create an optimized training configuration
based on your hardware and training goals.


✓ Detected GPU: NVIDIA GeForce RTX 4090 (24 GB VRAM)
Is this correct? [Y/n]: y

✓ Detected System RAM: 64 GB
Is this correct? [Y/n]: y

✓ Detected storage type: NVME
Is this correct? [Y/n]: y

How many images in your dataset? (default: 1000): 2500

Typical image resolution (e.g., 512, 768, 1024)? (default: 1024): 1024

What is your training goal?
  1. LoRA (Low-Rank Adaptation)
  2. Full Fine-tuning
  3. DreamBooth
  4. Textual Inversion
  5. Other/Custom
Choice [1-5] (default: 1): 1

How many epochs do you plan to train? (default: 10): 15

Optimization level
  1. Conservative (safe defaults)
  2. Balanced (recommended)
  3. Aggressive (maximum performance)
Choice [1-3] (default: 2): 3

Where to save the generated config? (default: config/optimized.yaml): config/my_lora.yaml

======================================================================
Calculating optimal configuration...

→ Using in-memory cache (~15.0GB, fits in RAM)

✓ Configuration saved to: config/my_lora.yaml

======================================================================
CONFIGURATION SUMMARY
======================================================================

Batch Size: 4
  → Auto-scaling enabled: 2-8

DataLoader Workers: 8
  → Persistent workers enabled

Caching Strategy: In-memory (shared)

GPU Prefetching: 2 batches

Learning Rate: 0.0001
Training Steps: 9375
Mixed Precision: bf16

======================================================================

Next steps:
  1. Review and customize: config/my_lora.yaml
  2. Update 'folder_path' to point to your dataset
  3. Adjust network settings for your model
  4. Run training: python run.py config/my_lora.yaml

```

## Generated Configuration

The wizard creates a complete YAML configuration with:

- **Hardware-optimized settings** - batch size, workers, memory usage
- **Explanatory comments** - why each setting was chosen
- **Smart defaults** - based on your hardware profile
- **Performance optimizations** - caching, prefetching, auto-scaling

### Example Generated Config

```yaml
# AI Toolkit - Optimized Training Configuration
# Generated by Interactive Config Wizard
#
# Hardware Profile:
#   GPU VRAM: 24 GB
#   System RAM: 64 GB
#   Storage: NVME
#
# Training Profile:
#   Dataset size: 2500 images
#   Resolution: 1024x1024
#   Goal: LoRA (Low-Rank Adaptation)
#   Epochs: 15
#   Optimization: Aggressive (maximum performance)

job: extension
config:
  name: optimized_training
  process:
    - type: sd_trainer
      training_folder: output

      # Network architecture
      network:
        type: lora
        linear: 16
        linear_alpha: 16

      # Training parameters
      train:
        # Batch size optimized for 24GB VRAM
        batch_size: 4

        # Smart batch size scaling - automatically adjusts for optimal GPU usage
        auto_scale_batch_size: true
        min_batch_size: 2
        max_batch_size: 8
        batch_size_warmup_steps: 100

        steps: 9375
        gradient_accumulation_steps: 1
        lr: 0.0001
        optimizer: adamw8bit
        dtype: bf16

      # Dataset configuration
      datasets:
        - folder_path: '/path/to/your/dataset'
          caption_ext: txt
          resolution: 1024

          # DataLoader workers - 8 workers optimized for 64GB RAM
          num_workers: 8
          # Keep workers alive between epochs - saves 16-20+ seconds per epoch
          persistent_workers: true

          # In-memory latent caching - fastest but uses RAM
          # Workers share cached data via shared memory (TODO #2)
          cache_latents: true
          cache_latents_to_disk: false

          # GPU prefetching - async load next 2 batch(es) to GPU
          # Reduces GPU idle time, especially beneficial for NVME storage
          gpu_prefetch_batches: 2
```

## Optimization Levels

### Conservative (Safe Defaults)
- **Best for:** Limited hardware, first-time users
- **Features:**
  - Lower batch sizes
  - Fewer workers
  - No auto-scaling
  - Minimal GPU prefetching
  - Gradient checkpointing on low VRAM

### Balanced (Recommended)
- **Best for:** Most users
- **Features:**
  - Moderate batch sizes
  - Balanced worker count
  - Smart caching based on dataset size
  - GPU prefetching on slower storage
  - Good performance without risk

### Aggressive (Maximum Performance)
- **Best for:** High-end hardware, experienced users
- **Features:**
  - Auto-scaling batch size
  - Maximum workers (RAM permitting)
  - Aggressive GPU prefetching
  - Persistent workers
  - All optimizations enabled

## How It Optimizes

### Batch Size Calculation
- Based on GPU VRAM and image resolution
- Conservative: safer sizes to avoid OOM
- Aggressive: enables auto-scaling to find optimal size

### Worker Count
- Calculated from available RAM
- Heuristic: ~1 worker per 8GB RAM
- Limited by CPU core count
- Adjusted by optimization level

### Caching Strategy
- Estimates cache size from dataset
- **In-memory cache:** Fast, uses RAM, shared across workers
- **Disk cache:** Slower, minimal RAM, memory-mapped
- Auto-selects based on available RAM

### GPU Prefetching
- Enabled for slower storage (HDD > SSD > NVMe)
- Aggressive mode enables even on fast storage
- Reduces GPU idle time
- More batches prefetched for slower storage

### Learning Rate
- Automatically set based on training goal:
  - LoRA: 1e-4
  - DreamBooth: 5e-6
  - Textual Inversion: 5e-3
  - Full fine-tune: 1e-5

## Hardware Detection

The wizard automatically detects:

- **GPU model and VRAM** - via `nvidia-smi`
- **System RAM** - via `psutil`
- **Storage type** - via `lsblk` (Linux)

If detection fails or is incorrect, you can manually enter values.

## Common Use Cases

### Case 1: New User with RTX 3090
```
GPU: 24GB
RAM: 32GB
Dataset: 1000 images
Goal: LoRA
→ Recommendation: Balanced optimization
```

Generated config:
- Batch size: 4
- Workers: 4
- Memory cache
- GPU prefetch: 1 batch

### Case 2: Experienced User with RTX 4090
```
GPU: 24GB
RAM: 128GB
Dataset: 5000 images
Goal: Full fine-tune
→ Recommendation: Aggressive optimization
```

Generated config:
- Batch size: 4, auto-scaling to 8
- Workers: 8
- Memory cache
- GPU prefetch: 2 batches
- Persistent workers

### Case 3: Budget Setup with RTX 3060
```
GPU: 12GB
RAM: 16GB
Dataset: 500 images
Goal: DreamBooth
→ Recommendation: Conservative optimization
```

Generated config:
- Batch size: 2
- Workers: 2
- Disk cache (save RAM)
- Gradient checkpointing
- Minimal prefetching

## Advanced Usage

### Non-Interactive Mode (Future)

You can also use the wizard programmatically:

```python
from toolkit.config_wizard import ConfigWizard

wizard = ConfigWizard()

# Set answers programmatically
wizard.answers = {
    'gpu_vram': 24,
    'ram_gb': 64,
    'storage': 'nvme',
    'dataset_size': 2500,
    'resolution': 1024,
    'training_goal': 'LoRA (Low-Rank Adaptation)',
    'epochs': 15,
    'optimization_level': 'Aggressive (maximum performance)',
    'output_path': 'config/my_config.yaml'
}

# Generate and save
wizard.calculate_optimal_config()
wizard.save_config()
```

## Troubleshooting

### GPU Not Detected
- Ensure `nvidia-smi` is in PATH
- Run `nvidia-smi` manually to check
- You can manually enter VRAM if detection fails

### Incorrect Storage Type
- Detection may fail on some systems
- Manually select correct type when prompted
- Storage type affects prefetching settings

### Generated Config Doesn't Work
- The wizard generates a template
- You must update `folder_path` to your dataset
- Network settings may need adjustment for your model
- Consider starting with Conservative optimization

## Tips for Best Results

1. **Be honest about hardware** - incorrect values lead to suboptimal configs
2. **Start with Balanced** - then adjust based on results
3. **Review the config** - comments explain each setting
4. **Test and iterate** - monitor GPU/RAM usage during training
5. **Save configs** - name them descriptively for different experiments

## Integration with AI Toolkit Features

The wizard leverages all major AI Toolkit optimizations:

- **TODO #1:** Lazy model loading (automatic)
- **TODO #2:** Shared memory caching (when using memory cache)
- **TODO #3:** Memory-mapped storage (when using disk cache)
- **TODO #4:** Persistent workers (multi-epoch training)
- **TODO #5:** GPU prefetching (configurable)
- **TODO #9:** Smart batch size scaling (aggressive mode)

## What You Need to Customize

After generation, update these in the config:

1. **folder_path** - Point to your actual dataset
2. **Network settings** - Adjust for your model architecture
3. **Model path** - If loading a base model
4. **Training folder** - Where to save outputs
5. **Network type** - LoRA, full, etc.

## Next Steps

After generating your config:

1. **Review:** Open the YAML and read the comments
2. **Customize:** Update paths and model settings
3. **Test:** Run a short training session
4. **Monitor:** Watch GPU/RAM usage
5. **Iterate:** Adjust settings based on results

## Getting Help

If the wizard doesn't work for your use case:

- Check the [main documentation](../README.md)
- Review [TODOs.md](../TODOs.md) for optimization details
- File an issue with your hardware specs

## Examples

Run tests to see the wizard in action:

```bash
python test_config_wizard.py
```

This runs the test suite and shows example configurations for different scenarios.
