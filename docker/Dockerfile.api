FROM nvidia/cuda:12.8.1-devel-ubuntu24.04

LABEL authors="jaret"

# Set noninteractive to avoid timezone prompts
ENV DEBIAN_FRONTEND=noninteractive

# ref https://en.wikipedia.org/wiki/CUDA
ENV TORCH_CUDA_ARCH_LIST="8.0 8.6 8.9 9.0 10.0 12.0"

# Install dependencies including those needed for the API server
RUN apt-get update && apt-get install --no-install-recommends -y \
    git \
    curl \
    build-essential \
    cmake \
    wget \
    python3.12 \
    python3-pip \
    python3-dev \
    python3-setuptools \
    python3-wheel \
    python3-venv \
    ffmpeg \
    tmux \
    htop \
    nvtop \
    python3-opencv \
    openssh-client \
    openssh-server \
    openssl \
    rsync \
    unzip \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app/ai-toolkit

# Set aliases for python and pip
RUN ln -s /usr/bin/python3 /usr/bin/python

# Install pytorch with stable versions (before copying project to leverage docker cache)
RUN pip install --no-cache-dir torch==2.9.1 torchvision==0.24.1 torchaudio==2.9.1 --index-url https://download.pytorch.org/whl/cu128 --break-system-packages

# Copy the project files
COPY . /app/ai-toolkit/

# Install Python dependencies
RUN pip install --no-cache-dir --break-system-packages -r requirements.txt && \
    pip install setuptools==69.5.1 --no-cache-dir --break-system-packages

# Pre-cache HuggingFace configs for offline from_single_file support (SD1/SD2/SDXL).
# Only config/tokenizer files — not multi-GB weights.
# Uses hf-mirror.com because HuggingFace has blocked our build server.
# SD2 repos are gated — pass --secret id=hf-token,env=HF_TOKEN to include them.
#
# IMPORTANT: diffusers' from_single_file() uses DIFFUSERS_DEFAULT_PIPELINE_PATHS which maps
# SD v1 to "stable-diffusion-v1-5/stable-diffusion-v1-5" (community org, NOT runwayml).
# kohya_model_util.py still references "runwayml/stable-diffusion-v1-5" so we cache both.
RUN HF_ENDPOINT=https://hf-mirror.com HF_HOME=/opt/hf-cache huggingface-cli download \
        stable-diffusion-v1-5/stable-diffusion-v1-5 --include "*.json" "*.txt" "*.model" && \
    HF_ENDPOINT=https://hf-mirror.com HF_HOME=/opt/hf-cache huggingface-cli download \
        runwayml/stable-diffusion-v1-5 --include "*.json" "*.txt" "*.model" && \
    HF_ENDPOINT=https://hf-mirror.com HF_HOME=/opt/hf-cache huggingface-cli download \
        stabilityai/stable-diffusion-xl-base-1.0 --include "*.json" "*.txt" "*.model" && \
    HF_ENDPOINT=https://hf-mirror.com HF_HOME=/opt/hf-cache huggingface-cli download \
        openai/clip-vit-large-patch14 --include "*.json" "*.txt" "*.model"
# SD2 is gated on HuggingFace — download only if an HF token is provided
RUN --mount=type=secret,id=hf-token,required=false \
    if [ -f /run/secrets/hf-token ]; then \
        HF_TOKEN=$(cat /run/secrets/hf-token) \
        HF_ENDPOINT=https://hf-mirror.com HF_HOME=/opt/hf-cache huggingface-cli download \
            stabilityai/stable-diffusion-2-1 --include "*.json" "*.txt" "*.model"; \
    fi

COPY docker/entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# Expose port for API server
EXPOSE 8000

WORKDIR /app/ai-toolkit

# Start the API server via entrypoint (seeds HF cache before starting uvicorn)
ENTRYPOINT ["/entrypoint.sh"]
CMD ["python", "-m", "uvicorn", "api_server.app:app", "--host", "0.0.0.0", "--port", "8000"]
